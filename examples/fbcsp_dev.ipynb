{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ezmsg.util.messagelogger import MessageDecoder\n",
    "\n",
    "from hololight.shallowfbcspnet import ShallowFBCSPNet, get_output_shape\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = []\n",
    "data_fname = Path( '..' ) / 'recordings' / 'traindata.txt'\n",
    "with open( data_fname, 'r' ) as data_file:\n",
    "    samples = [ json.loads( line, cls = MessageDecoder ) for line in data_file ]\n",
    "\n",
    "eeg_trials = np.array( [ s[ 'sample' ][ 'data' ] for s in samples ] )\n",
    "eeg_labels = np.array( [ s[ 'trigger' ][ 'value' ] for s in samples ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n",
      "Sequential(\n",
      "  (ensuredims): Ensure4d()\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d(1, 40, kernel_size=(25, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d(40, 40, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_nonlin): Expression(expression=square)\n",
      "  (pool): AvgPool2d(kernel_size=(75, 1), stride=(1, 1), padding=0)\n",
      "  (pool_nonlin): Expression(expression=safe_log)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (conv_classifier): Conv2d(40, 2, kernel_size=(30, 1), stride=(1, 1), dilation=(15, 1))\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ")\n",
      "torch.Size([1, 2, 167])\n"
     ]
    }
   ],
   "source": [
    "net = ShallowFBCSPNet(\n",
    "    in_chans = 8,\n",
    "    n_classes = 2,\n",
    "    input_time_length = 1000,\n",
    "    cropped_training = True,\n",
    "    n_filters_time = 40,\n",
    "    filter_time_length = 25,\n",
    "    n_filters_spat = 40,\n",
    "    pool_time_length = 75,\n",
    "    pool_time_stride = 15,\n",
    "    conv_nonlin = 'square',\n",
    "    pool_mode = 'mean',\n",
    "    pool_nonlin = 'safe_log',\n",
    "    split_first_layer = True,\n",
    "    batch_norm = True,\n",
    "    batch_norm_alpha = 0.1,\n",
    "    drop_prob = 0.5,\n",
    ")\n",
    "\n",
    "model = net.construct()\n",
    "print( model )\n",
    "print( get_output_shape( \n",
    "    model, \n",
    "    in_chans = net.in_chans, \n",
    "    input_window_samples = 700\n",
    ") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader, \n",
    "    random_split, \n",
    "    RandomSampler,\n",
    "    WeightedRandomSampler\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from shallowfbcspnet import ShallowFBCSPNet\n",
    "\n",
    "from typing import (\n",
    "    Optional,\n",
    "    List,\n",
    "    Tuple\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( f\"Using Device: {device}\" )\n",
    "\n",
    "if device == 'cuda':\n",
    "    cuda_id = 1 # torch.cuda.current_device()\n",
    "    torch.cuda.set_device( cuda_id )\n",
    "    print( f\"ID of current CUDA device:{ torch.cuda.current_device() }\" )\n",
    "    print( f\"Name of current CUDA device: { torch.cuda.get_device_name( cuda_id ) }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FBCSPDataset( Dataset ):\n",
    "    \"\"\" \n",
    "    This is a wrapper around a xr.DataArray to generate a multi-class problem \n",
    "    Given a data array of epochs with a --\n",
    "        * trial dimension upon which labels exist\n",
    "        * feat/channel dimension,\n",
    "        * time dimension\n",
    "        \n",
    "    This dataset allows you to iterate over the dataset and produces one-indexed\n",
    "    labels corresponding to self.label_dict\n",
    "    \n",
    "    The idea is to provide trials that are centered around some t_0 = 0 corresponding \n",
    "    to a trial onset and a stretch of data before t_0 = 0 corresponds to a null event\n",
    "    which is assigned a label of 0.  As such, len( dataset ) will actually result in\n",
    "    len( da[ trial_dim ] ) * 2.  Every other trial is a null trial\n",
    "    \"\"\"\n",
    "    \n",
    "    da: xr.DataArray # 3D DataArray incl. feat_dim, trial_dim, and time_dim\n",
    "    feat_dim: str = 'feat' # Features dimension (You may need to stack some dims to make this)\n",
    "    trial_dim: str = 'event' # Trial dimension - Should be the dimension along which labels exist\n",
    "    time_dim: str = 'time' # Time dimension -- each epoch should be a short stretch of time\n",
    "    label_coord: str = 'label' # Coordinate on trial_dim that contains labels\n",
    "    t_min: float = -0.01 # Time labels to sub-select the input data such that ---\n",
    "    t_max: float = 0.05 # --- trials.sel( time = slice( t_min, t_max ) ) => condition (one-indexed label)\n",
    "    add_null_class: bool = True # If True, trials.sel( time = slice( -t_max, -t_min ) ) => null (0)\n",
    "    crops: int = 2 # Perform data augmentation by using sliding windows across epochs\n",
    "\n",
    "    def __post_init__( self ):\n",
    "        self.da = self.da.transpose( self.trial_dim, self.feat_dim, self.time_dim )\n",
    "        self.conditions = self.da.sel( time = slice( self.t_min, self.t_max ) )\n",
    "        self.n_time = len( self.conditions[ self.time_dim ] )\n",
    "        self.n_feat = len( self.conditions[ self.feat_dim ] )\n",
    "        \n",
    "        self.label_dict = { label: idx for idx, label in enumerate( np.unique( self.da[ self.label_coord ] ) ) }\n",
    "        \n",
    "        if self.add_null_class:\n",
    "            null_t_min_idx = np.abs( self.da[ self.time_dim ] - ( -self.t_min ) ).argmin( self.time_dim ).item()\n",
    "            self.nulls = self.da.isel( time = slice( null_t_min_idx - self.n_time, null_t_min_idx ) )\n",
    "            self.label_dict = { l: ( i + 1 ) for l, i in self.label_dict.items() }\n",
    "            self.label_dict[ 'null' ] = 0\n",
    "            \n",
    "        if self.crops > 0:\n",
    "            self.n_time = self.n_time - self.crops\n",
    "        \n",
    "        self.prefetch_label = []\n",
    "        self.prefetch_data = []\n",
    "        for idx in range( len( self ) ):\n",
    "    \n",
    "            trial_source = self.conditions\n",
    "            if self.add_null_class:\n",
    "                trial_source: xr.DataArray = self.conditions if ( idx % 2 ) == 0 else self.nulls\n",
    "                idx: int = int( idx // 2 )\n",
    "\n",
    "            crop_idx = 0\n",
    "            if self.crops > 0:\n",
    "                crop_idx = idx % ( self.crops + 1 )\n",
    "                idx = int( idx // ( self.crops + 1 ) )\n",
    "\n",
    "            trial = trial_source.isel( { \n",
    "                self.trial_dim: idx, \n",
    "                self.time_dim: slice( crop_idx, crop_idx + self.n_time ) \n",
    "            } )\n",
    "\n",
    "            label = 0\n",
    "            if trial_source is self.conditions:\n",
    "                label = self.label_dict[ trial[ self.label_coord ].item() ]\n",
    "            self.prefetch_label.append( torch.tensor( label ) )\n",
    "            self.prefetch_data.append( torch.tensor( trial.values.astype( np.float32 ) ) )\n",
    "            \n",
    "        self.prefetch_data = torch.cat( [ d[ None, ... ] for d in self.prefetch_data ] )\n",
    "        self.prefetch_label = torch.tensor( self.prefetch_label ) \n",
    "                        \n",
    "    def __len__( self ) -> int:\n",
    "        n = len( self.da[ self.trial_dim ] )\n",
    "        n = n * ( 2 if self.add_null_class else 1 )\n",
    "        if self.crops > 0:\n",
    "            n = n * ( self.crops + 1 )\n",
    "        return n\n",
    "\n",
    "    def __getitem__( self, idx: int ) -> Tuple[ torch.tensor, int ]:           \n",
    "        return ( \n",
    "            self.prefetch_data[ idx, ... ], \n",
    "            self.prefetch_label[ idx, ... ] \n",
    "        )\n",
    "    \n",
    "feats = filt_phase_vel\n",
    "feats = feats \\\n",
    "    .stack( feat = [ 'x', 'y' ] ) \\\n",
    "#     .sel( event = filt_pos.label != 'C1' ) \\\n",
    "\n",
    "\n",
    "train_ratio = 0.75\n",
    "train_da, test_da = [], []\n",
    "for label, label_da in feats.groupby( 'label' ):\n",
    "    train_events = int( train_ratio * len( label_da.event ) )\n",
    "    indices = np.arange( len( label_da.event ) )\n",
    "    np.random.shuffle( indices )\n",
    "    train_indices = indices[ :train_events ]\n",
    "    test_indices = indices[ train_events: ]\n",
    "    train_da.append( label_da.isel( event = train_indices ) )\n",
    "    test_da.append( label_da.isel( event = test_indices ) )\n",
    "train_da = xr.concat( train_da, 'event' )\n",
    "test_da = xr.concat( test_da, 'event' )\n",
    "\n",
    "# We have to split this way because crops can contaminate test dataset if split randomly hereafter\n",
    "train_dset = FBCSPDataset( train_da, add_null_class = True )\n",
    "test_dset = FBCSPDataset( test_da, add_null_class = True )\n",
    "\n",
    "print( f'Train dset: { len( train_dset ) } examples of { train_dset.n_feat } pixels x { train_dset.n_time } time points' )\n",
    "for lab, count in zip( *torch.unique( train_dset.prefetch_label, return_counts = True ) ):\n",
    "    print( f'\\t*{ count } { lab.item() } examples' )\n",
    "    \n",
    "print( f'Test dset: { len( test_dset ) } examples of { test_dset.n_feat } pixels x { test_dset.n_time } time points' )\n",
    "for lab, count in zip( *torch.unique( test_dset.prefetch_label, return_counts = True ) ):\n",
    "    print( f'\\t*{ count } { lab.item() } examples' )\n",
    "    \n",
    "print( train_dset.label_dict )\n",
    "print( test_dset.label_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "max_epochs = 300\n",
    "batch_size = 32\n",
    "weight_decay = 0.0\n",
    "\n",
    "model_definition = ShallowFBCSPNet( \n",
    "    train_dset.n_feat, \n",
    "    len( train_dset.label_dict ), \n",
    "    input_time_length = train_dset.n_time, \n",
    "    final_conv_length = 'auto',\n",
    "    split_first_layer = True,\n",
    "    filter_time_length = 10,\n",
    "    n_filters_time = 40,\n",
    "    n_filters_spat = 40,\n",
    "    pool_mode = 'mean',\n",
    "    pool_time_length = 10, #25\n",
    "    pool_time_stride = 2,\n",
    "    drop_prob = 0.5,\n",
    "    batch_norm = True,\n",
    "    batch_norm_alpha = 0.1\n",
    ")\n",
    "\n",
    "model = model_definition.construct()\n",
    "model = model.to( device )\n",
    "print( model )\n",
    "\n",
    "model_parameters = filter( lambda p: p.requires_grad, model.parameters() )\n",
    "params = sum( [ np.prod( p.size() ) for p in model_parameters ] )\n",
    "print( f'Model has {params} trainable parameters' )\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW( \n",
    "    model.parameters(), \n",
    "    lr = learning_rate, \n",
    "    weight_decay = weight_decay \n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( optimizer, T_max = max_epochs / 1 )\n",
    "\n",
    "best_loss = None\n",
    "best_loss_epoch = None\n",
    "\n",
    "train_loss, test_loss, test_accuracy = [], [], []\n",
    "lr = []\n",
    "\n",
    "epoch_itr = tqdm( range( max_epochs ) )\n",
    "\n",
    "# Calculate weights for class balancing\n",
    "classes, counts = torch.unique( train_dset.prefetch_label, return_counts = True )\n",
    "weights = { cl.item(): 1.0 / co.item() for cl, co in zip( classes, counts ) }\n",
    "weights = [ weights[ lab.item() ] for lab in train_dset.prefetch_label ]\n",
    "\n",
    "for epoch in epoch_itr:\n",
    "\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    for train_feats, train_labels in DataLoader(\n",
    "        train_dset, \n",
    "        batch_size = batch_size, \n",
    "        sampler = WeightedRandomSampler( weights, len( train_dset ), replacement = False ),\n",
    "        pin_memory = True if device == 'cuda' else False,\n",
    "    ):\n",
    "        pred = model( train_feats.to( device ) )\n",
    "        loss = loss_fn( pred, train_labels.to( device ) )\n",
    "        train_loss_batches.append( loss.cpu().item() )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    lr.append( scheduler.get_last_lr()[0] )\n",
    "    train_loss.append( np.mean( train_loss_batches ) )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        test_loss_batches = []\n",
    "        for test_feats, test_labels in DataLoader(\n",
    "            test_dset, \n",
    "            batch_size = batch_size, \n",
    "            pin_memory = True if device == 'cuda' else False\n",
    "        ):\n",
    "            output = model( test_feats.to( device ) )\n",
    "            test_loss_batches.append( loss_fn( output, test_labels.to( device ) ).cpu().item() )\n",
    "            accuracy += ( output.argmax( axis = 1 ).cpu() == test_labels ).sum().item()\n",
    "\n",
    "        test_loss.append( np.mean( test_loss_batches ) )\n",
    "        test_accuracy.append( accuracy / len( test_dset ) )\n",
    "        \n",
    "acc_str = f'Test Accuracy: {test_accuracy[-1] * 100.0:0.2f}%'\n",
    "\n",
    "fig, ax = plt.subplots( dpi = 100, figsize = ( 6.0, 4.0 ) )\n",
    "ax.plot( train_loss, label = 'Train' )\n",
    "ax.plot( test_loss, label = 'Test' )\n",
    "ax.plot( test_accuracy, label = 'Test Accuracy' )\n",
    "ax.plot( lr, label = 'Learning Rate' )\n",
    "ax.legend()\n",
    "ax.set_yscale( 'log' )\n",
    "ax.set_xlabel( 'Epoch' )\n",
    "ax.axhline( 1, color = 'k' )\n",
    "ax.set_title( acc_str )\n",
    "\n",
    "# out_train = input_dir / f'{tag}_train.png'\n",
    "# fig.savefig( out_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "output = [ \n",
    "    ( model( test_feats.to( device ) ).cpu().argmax( axis = 1 ), test_labels )\n",
    "    for test_feats, test_labels \n",
    "    in DataLoader( test_dset, batch_size = batch_size ) \n",
    "]\n",
    "\n",
    "decode, test_y = zip( *output )\n",
    "test_y = torch.cat( test_y, axis = 0 )\n",
    "decode = torch.cat( decode, axis = 0 )\n",
    "\n",
    "classes = list( sorted( test_dset.label_dict.values() ) ) \n",
    "rev_dict = { v: k for k, v in test_dset.label_dict.items() }\n",
    "class_labels = [ rev_dict[ c ] for c in classes ]\n",
    "confusion = np.zeros( ( len( classes ), len( classes ) ) )\n",
    "for true_idx, true_class in enumerate( classes ):\n",
    "    class_trials = np.where( test_y == true_class )[0]\n",
    "    for pred_idx, pred_class in enumerate( classes ):\n",
    "        num_preds = ( decode[ class_trials ] == pred_class ).sum().item()\n",
    "        confusion[ true_idx, pred_idx ] = num_preds / len( class_trials )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( dpi = 100 )\n",
    "corners = np.arange( len( classes ) + 1 ) - 0.5\n",
    "im = ax.pcolormesh( \n",
    "    corners, corners, confusion, alpha = 0.5,\n",
    "    cmap = plt.cm.Blues, vmin = 0.0, vmax = 1.0\n",
    ")\n",
    "\n",
    "for row_idx, row in enumerate( confusion ):\n",
    "    for col_idx, freq in enumerate( row ):\n",
    "        ax.annotate( \n",
    "            f'{freq:0.2f}', ( col_idx, row_idx ), \n",
    "            ha = 'center', va = 'center' \n",
    "        )\n",
    "\n",
    "ax.set_aspect( 'equal' )\n",
    "ax.set_xticks( classes )\n",
    "ax.set_yticks( classes )\n",
    "ax.set_xticklabels( class_labels )\n",
    "ax.set_yticklabels( class_labels )\n",
    "ax.set_ylabel( 'True Class' )\n",
    "ax.set_xlabel( 'Predicted Class' )\n",
    "ax.invert_yaxis( )\n",
    "fig.colorbar( im )\n",
    "ax.set_title( acc_str )\n",
    "\n",
    "# out_accuracy = input_dir / f'{tag}_acc.png'\n",
    "# fig.savefig( out_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_definition': model_definition,\n",
    "    'fs': trials.fs, \n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "\n",
    "out_checkpoint = f'FBCSP.checkpoint'\n",
    "torch.save( checkpoint, out_checkpoint )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed258a9152e085ee7311268e121aa17c6767ee64cbeeaf73a64058897dead45f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
